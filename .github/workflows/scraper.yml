import sqlite3
import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime
from utils import extract_price

DB_NAME = "scraping.db"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0 Safari/537.36"
    ),
    "Accept-Language": "pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7",
}


# ========================================================
# GARANTIR SCHEMA DA TABELA
# ========================================================
def ensure_schema():
    conn = sqlite3.connect(DB_NAME)
    cur = conn.cursor()

    cur.execute("""
        CREATE TABLE IF NOT EXISTS products (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT NOT NULL,
            name TEXT,
            price REAL,
            image_url TEXT,
            last_update TEXT
        )
    """)

    conn.commit()
    conn.close()

def fetch_product_data(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=20)

        if response.status_code != 200:
            print(f"‚ùå Erro ao acessar {url} (HTTP {response.status_code})")
            return None

        soup = BeautifulSoup(response.text, "html.parser")

        # NOME
        title = soup.find(id="productTitle")
        name = title.get_text(strip=True) if title else "Nome n√£o encontrado"

        # PRE√áO
        price_elem = soup.find("span", {"class": "a-offscreen"})
        price = extract_price(price_elem.text) if price_elem else None

        # IMAGEM
        img = soup.find("img", {"id": "landingImage"})
        image_url = img["src"] if img else None

        return {
            "name": name,
            "price": price,
            "image_url": image_url,
        }

    except Exception as e:
        print(f"‚ö† Erro ao processar {url}: {e}")
        return None


# ========================================================
# ATUALIZAR PRODUTO NO BANCO
# ========================================================
def update_product(product_id, data):
    conn = sqlite3.connect(DB_NAME)
    cur = conn.cursor()

    cur.execute("""
        UPDATE products
        SET name = ?, price = ?, image_url = ?, last_update = ?
        WHERE id = ?
    """, (
        data["name"],
        data["price"],
        data["image_url"],
        datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        product_id
    ))

    conn.commit()
    conn.close()


# ========================================================
# BUSCAR TODOS OS PRODUTOS E ATUALIZAR
# ========================================================
def run_scraper():
    print("üöÄ Iniciando scraper autom√°tico...")

    ensure_schema()

    conn = sqlite3.connect(DB_NAME)
    cur = conn.cursor()

    cur.execute("SELECT id, url FROM products")
    products = cur.fetchall()

    conn.close()

    if not products:
        print("‚ö† Nenhum produto cadastrado.")
        return

    for product_id, url in products:
        print(f"üîç Atualizando ID {product_id}: {url}")

        data = fetch_product_data(url)

        if data:
            update_product(product_id, data)
            print(f"‚úÖ Produto {product_id} atualizado.")
        else:
            print(f"‚ùå Falha ao atualizar {url}")

    print("üéâ Scraper finalizado!")


# ========================================================
# EXECUTAR DIRETO DO TERMINAL / GITHUB ACTIONS
# ========================================================
if __name__ == "__main__":
    run_scraper()
